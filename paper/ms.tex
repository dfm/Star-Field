\documentclass[letterpaper, 11pt]{article}
\usepackage{graphicx}	% For figures
\usepackage{natbib}	% For citet and citep
\usepackage{amsmath}	% for \iint
\usepackage{bbm}	% for blackboard bold numbers
\usepackage[left=3cm,top=3cm,right=3cm]{geometry}

\renewcommand{\topfraction}{0.85} \renewcommand{\textfraction}{0.1} 
\parindent=0cm
\newcommand{\sparse}{\texttt{Sparse}}
\newcommand{\cluster}{\texttt{Cluster}}
\newcommand{\crowded}{\texttt{Crowded}}

\title{Probabilistic Catalogs from Crowded Star Fields}
\author{Brendon J. Brewer$^{1, 2, ^*}$, David W. Hogg$^{3}$,
and Daniel Foreman-Mackey$^{3}$ \\
\\
\small
$^1$ Department of Physics, University of California, Santa Barbara,\\
\small
CA 93106, USA \\
\small
$^2$ Department of Statistics, The University of Auckland,\\
\small
Private Bag 92019, Auckland 1142, New Zealand \\
\small
$^3$ Center for Cosmology and Particle Physics, Department of Physics,
New York University,\\
\small
4 Washington Place, New York, NY, 10003, USA\\
\small
$^*$\texttt{bj.brewer@auckland.ac.nz}
}

\begin{document}
\maketitle

\section{Introduction}

A fundamental problem in astronomy is the construction of {\it catalogs} from
raw image data. {\bf The catalogs are then used for...}
Standard tools for generating catalogs include \texttt{SExtractor}
\citep{sextractor}, {\bf more refs}.

However, standard methods for constructing catalogs can have difficulty in some
challenging situations. For example, when multiple sources overlap partially
or completely, it can be difficult to determine how many sources are present,
and how much flux belongs to each source. In principle, uncertainty should be
taken into account -- instead of simply estimating the position and flux of each
object, we should fully describe the fact that sometimes we are not certain of
the position and flux of each source.

Essentially, the creation of a catalog is an attempt to answer the question
``given the image we have obtained, what objects are present in the field and
what are their properties?''. This motivates a probabilistic (Bayesian)
approach to making catalogs.

\citet{2011MNRAS.415.3462F} have attempted this, by replacing the complex
$N$-component model with a single-component model, and using the
single-component
results to reconstruct what the $N$-component results would have been. However,
this will not work well in situations where there is significant confusion
(i.e. two or more sources overlap) and also does not allow for unknown $N$,
which is the general situation. What we really need is a variable dimension
model, where $N$ is an unknown quantity to be inferred from the data.
The computational implementation of these models will require tools such as reversible jump Markov Chain Monte Carlo \citep{rjmcmc}.

\section{Bayesian Inference}
To quantitatively model uncertainties, Bayesian Inference is the appropriate
framework \citep{cox, jaynes, caticha}. Suppose there exist unknown parameters (denoted
collectively by $\theta$) and we expect to obtain some data $D$. Our prior
knowledge (or uncertainty) about the parameters is modelled by a prior
probability distribution:
\begin{equation}
p(\theta)
\end{equation}
We also model how the parameters give rise to the data, via a {\it sampling
distribution}:
\begin{equation}
p(D|\theta)
\end{equation}
Despite the singular, the sampling distribution is actually a family of
probability distributions, one for each possible value of $\theta$. Note that
the sampling distribution is also an assertion of a prior state of knowledge.
In this case it models our prior information about the fact that the parameters
$\theta$ are going to influence the data in some way (otherwise this data
would be irrelevant).

When specific data $D^*$ are known, the prior gets updated to the posterior
via Bayes' rule:
\begin{eqnarray}
p(\theta|D=D^*) &\propto& p(\theta)p(D|\theta)|_{D=D^*} \\
&=& p(\theta)\mathcal{L}(\theta; D)
\end{eqnarray}
The modulating function is the {\it likelihood function}, which is the prior
probability of obtaining the actual data set $D^*$ as a function of the
parameters. As suggested by the above notation, it is obtained from the
sampling distribution with the actual data substituted in.

\section{The Specific Model for Stellar Fields}

\subsection{The Hypothesis Space and the Prior}
Our model makes the following assumptions. There are an unknown number of stars
$N$ in the field. Each star has an unknown
position $(x,y)$ in the plane of the sky, and an unknown flux $f$. We also
describe the {\it spatial distribution} of the stars by parameters
$\alpha$, and the distribution of fluxes (commonly known as the {\it luminosity
function}) by parameters $\beta$. In summary, the unknown parameters are:
\begin{eqnarray}
\theta = \left\{N, \alpha, \beta, \left\{x_i, y_i\right\}_{i=1}^N, 
\left\{f_i\right\}_{i=1}^N\right\}
\end{eqnarray}
The prior probability distribution for the unknown parameters can be factorized
using the product rule. With a variety of independence assumptions, the prior
can be written as:
\begin{eqnarray}
p(\theta) = p(N)p(\alpha)p(\beta)\prod_{i=1}^N p(x_i, y_i | \alpha)
p(f_i | \beta) 
\end{eqnarray}
Here, we have assumed that the luminosity function does not depend on position.
We have also assumed independence of $N$, $\alpha$ and $\beta$, which means that
learning the value of one would not tell us anything about the values of the
others. Finally, the positions and fluxes of the stars come independently from
a common distribution. If we knew the spatial distribution and the luminosity
function of the stars, then the location and flux of a particular star would
not tell us anything about the location and flux of another star. Really, this
is just a way of implementing exchangeability.

\subsection{The Sampling Distribution}
The sampling distribution is a probabilistic model for the process that
generates the data. {\bf PSF and Gaussian Noise}

\subsection{The Prior Distribution}
A model for the spatial distribution corresponds to a choice of functional
forms for the distributions $p(x_i, y_i | \alpha)$ and $p(f_i | \beta)$.

\section{Three Scenarios: Sparse, Cluster, and Crowded}
In this paper we discuss three test cases, named \sparse, \cluster
and \crowded. For the generation of the simulated
images, we used the following assumptions:

\begin{table}
\begin{center}
\begin{tabular}{ccccc}
Test Case & Parameter & Value \\
\hline
\sparse \\
\cluster \\
\crowded
\end{tabular}
\end{center}
\end{table}

\begin{figure}
\hspace{-1cm}
\includegraphics{Figures/test_cases.eps}
\end{figure}

\subsection{Sparse}
Sparse has a giant first-order phase transition. {\bf Nested Sampling is
mandatory!} Show the plot of it

\begin{figure}
\begin{center}
\includegraphics[scale=0.7]{Figures/likelihood.eps}
\end{center}
\end{figure}

\subsection{Cluster}
Was generated from non-uniform spatial distribution (obviously) but modelled
with a uniform one.

\subsection{Crowded}
Crowded is slow if the data have high S/N. Something like $e^5000$ compressions
are needed. Can do NS and not worry about equilibration if you are willing to
make a guess about the logXs of the levels.

\subsection{Toy Model}
For the first implementation I wrote at NYU, I used the following model and
priors:
\begin{itemize}
\item $N \sim \textnormal{Uniform}(0, 1, 2, ..., 1000)$ \\
\item The parameters of the spatial
distribution: $\alpha = \left\{x_c, y_c, \sigma\right\}$ \\
\item The spatial distribution: $p(x_i, y_i | \alpha)\sim\mathcal{N}\left(
\texttt{mean}=(x_c, y_c), \texttt{covariance}=
\left(\begin{array}{cc}\sigma^2 & 0 \\ 0 & \sigma^2\end{array}\right)\right)$ \\
\item Prior on $\alpha$: Uniform within image for $(x_c, y_c)$, logUniform for
$\sigma$ with max value $\sim$ image size.
\item The luminosity function: $p(f_i | \beta) \sim
\textnormal{Exponential}\left(\texttt{mean}=\beta\right)$ \\
\item Prior on $\beta$: logUniform with generous bounds.
\end{itemize}


\section{MCMC Implementation}
\subsection{Current}
I am using Diffusive Nested Sampling \citep{dnest}, which is really just
Metropolis-Hastings but targeting a fancy distribution instead of the actual
posterior, and then assigning importance weights when you want the posterior.
Nested Sampling is good because it is invariant under monotonic transformations
of the likelihood function. i.e. the exploration only depends on things like
whether one point is better than another, and not on how much better it is.

\section{A More Realistic Model}
\citep{2008ApJ...682..874K}

\appendix
\section{Pixel Convolved PSF}
Consider a ``true image'' $f_0(x, y)$ with infinite resolution. We now model how
this image gives rise to the observed data. First, it is convolved with a
PSF $g(\delta_x, \delta_y)$ (assumed normalized to 1) to give the blurred image:
\begin{eqnarray}
f_{\rm{blurred}}(x, y) &=& \iint f_0(x-\delta_x, y-\delta_y)
g(\delta_x, \delta_y) \, d\delta_x \, d\delta_y \label{blur}
\end{eqnarray}
Then, we observe the blurred image with a certain pixellation. Consider a pixel.
The true flux $F$ within the pixel is:
\begin{eqnarray}
F &=& \iint_{\rm pixel} f_{\rm blurred}(x, y) \,dx'\,dy'\\
&=& \iint f_{\rm blurred}(x, y)\mathbbm{1}\left[(x,y) \in \textnormal{pixel}
\right] \,dx\,dy \label{pixel}
\end{eqnarray}
Now consider $F$ as a function of the central location $(x_c, y_c)$ of the
pixel.




\subsubsection{Proposal Distributions}

\begin{table}
\begin{center}
\begin{tabular}{c|c|c}
Parameter & Proposal & Notes\\
\hline
$N$ & $N \to N + \delta_N$ & Generate $\delta_N$ new stars from prior given
$(\alpha, \beta)$\\
$N$ & $N \to N - \delta_N$ & Remove $\delta_N$ stars at random\\
$\alpha$ & $\alpha \to \alpha + \delta_\alpha$ & Move stars along with $\alpha$
\\
$\alpha$ & $\alpha \to \alpha + \delta_\alpha$ & Stars fixed, use logHastings
factor \\
$\beta$ & $\beta \to \beta + \delta_\beta$ & Move stars' fluxes along with
$\beta$\\
$\beta$ & $\beta \to \beta + \delta_\beta$ & Stars' fluxes fixed, use
logHastings factor \\
$(x,y)$ & $(x,y) \to (x,y)+(\delta_x, \delta_y)$ & Can move $>1$ star.
Include prior via logH \\
$f$ & $f \to f + \delta_f$ & Can move $>1$ stars' fluxes. Include prior
via logH
\end{tabular}
\end{center}
\caption{All $\delta$ parameters are drawn from multi-scale distibutions such
that the largest steps are of order the prior width, and the smallest steps
are of order $10^{-6}$ times the prior width.\label{proposals}}
\end{table}

See Table~\ref{proposals} for a list of current proposal distributions.

\section{Broken Pareto Distribution}
The Broken Pareto distribution is:
\begin{eqnarray}
p(x | \theta) &\propto&
\left\{
\begin{array}{lcr}
0, & & x < x_0 \\
x^{-\alpha_1 - 1}, & & x_0 \leq x \leq x_1 \\
x^{-\alpha_2 - 1}, & & x > x_1
\end{array}
\right.
\end{eqnarray}
where the parameters are $\theta = \{x_0, x_1, \alpha_0, \alpha_1\}$.
We will need the fully normalised version. Putting in two normalisers:
\begin{eqnarray}
p(x | \theta) &=&
\left\{
\begin{array}{lcr}
0, & & x < x_0 \\
Z_1^{-1}x^{-\alpha_1 - 1}, & & x_0 \leq x \leq x_1 \\
Z_2^{-1}x^{-\alpha_2 - 1}, & & x > x_1
\end{array}
\right.
\end{eqnarray}
The normalising constants can be obtained by enforcing the following
constraints. The first constraint is that the PDF should be continuous at
$x=x_1$:
\begin{eqnarray}
Z_1^{-1} x_1^{-\alpha_1 - 1} &=& Z_2^{-1} x_1^{-\alpha_2 - 1} \\
\implies Z_2 &=& x_1^{\alpha_1-\alpha_2}Z_1 \label{constraint1}
\end{eqnarray}
The second constraint is that the total probability is unity:
\begin{eqnarray}
\int p(x|\theta) \, dx &=& 1 \\
\implies
\int_{x_0}^{x_1} Z_1^{-1}x^{-\alpha_1 - 1} \, dx
+ \int_{x_1}^{\infty} Z_2^{-1}x^{-\alpha_2 - 1} \, dx &=& 1 \label{constraint2}
\end{eqnarray}
Combining equations~\ref{constraint1} and~\ref{constraint2} yields:
\begin{eqnarray}
Z_1 &=& \frac{x_0^{-\alpha_1} - x_1^{-\alpha_1}}{\alpha_1} + \frac{x_1^{-\alpha_1}}{\alpha_2}\\
Z_2 &=& x_1^{\alpha_1-\alpha_2}Z_1
\end{eqnarray}

%\subsection{Future Possibilities}
%{\bf Hogg}: {\it I think this problem is important enough and general enough
%that we should spend some time working on some ideas there.  If we can sample
%over image explanations, our powers in astronomy will be awesome
%beyond our wildest imaginings.}

%\subsection{Genetic Moves}
%Here I will sketch some ideas...




\begin{thebibliography}{99}
\bibitem[Bertin and 
Arnouts(1996)]{sextractor} Bertin, E., Arnouts, S.\ 1996.\ SExtractor: Software for source extraction.\ Astronomy and Astrophysics Supplement Series 117, 393-404.

\bibitem[\protect\citeauthoryear{Brewer, P{\'a}rtay, 
\& Cs{\'a}nyi}{2011}]{dnest} Brewer B.~J., P{\'a}rtay L.~B.,
Cs{\'a}nyi G., 2011, Statistics and Computing, 21, 4, 649-656. arXiv:0912.2380

\bibitem[Caticha(2009)]{caticha} Caticha, A.\ 2009.\ 
Quantifying Rational Belief.\ American Institute of Physics Conference 
Series 1193, 60-68. 

\bibitem[Cox(1946)]{cox} Cox, R.~T., 1946, Probability, Frequency, and Reasonable Expectation. 1946. American Journal of Physics 14 14, 1-13.

\bibitem[Feroz et al.(2011)]{2011MNRAS.415.3462F} Feroz, F., Balan, S.~T., 
Hobson, M.~P.\ 2011.\ Detecting extrasolar planets from stellar radial 
velocities using Bayesian evidence.\ Monthly Notices of the Royal 
Astronomical Society 415, 3462-3472. 

\bibitem[\protect\citeauthoryear{Green}{1995}]{rjmcmc}
Green, P.~J., 1995, Reversible Jump Markov Chain Monte Carlo Computation and Bayesian Model Determination, Biometrika 82 (4): 711–732.

\bibitem[Jaynes(2003)]{jaynes} Jaynes, E.~T., 2003, Probability Theory: The
Logic of Science, ISBN 0521592712, Cambridge University Press, June 2003.

\bibitem[Kelly et al.(2008)]{2008ApJ...682..874K} Kelly, B.~C., Fan, X., 
\& Vestergaard, M.\ 2008, ApJ, 682, 874 

\bibitem[Mackay(2003)]{mackay} Mackay, D.~J.~C., 2003, Information Theory,
Inference and Learning Algorithms, Cambridge University Press, UK.

\bibitem[\protect\citeauthoryear{Skilling}{1998}]{massinf} 
Skilling J., 1998, Massive Inference and Maximum Entropy, in Maximum Entropy 
and Bayesian Methods, Kluwer Academic Publishers, Dordrecht/Boston/London p.14
\end{thebibliography}

\end{document}

