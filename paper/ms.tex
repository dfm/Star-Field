\documentclass[letterpaper, 11pt]{article}
\usepackage{graphicx}	% For figures
\usepackage{natbib}	% For citet and citep
\usepackage{amsmath}	% for \iint
\usepackage{bbm}	% for blackboard bold numbers
\usepackage[left=3cm,top=3cm,right=3cm]{geometry}

\renewcommand{\topfraction}{0.85} \renewcommand{\textfraction}{0.1} 
\parindent=0cm
\newcommand{\sparse}{\texttt{Sparse}}
\newcommand{\cluster}{\texttt{Cluster}}
\newcommand{\crowded}{\texttt{Crowded}}

\title{Probabilistic Catalogs from Crowded Star Fields}
\author{Brendon J. Brewer$^{1,*}$, David W. Hogg$^{2}$,
and Daniel Foreman-Mackey$^{2}$ \\
\\
\small
$^1$ Department of Statistics, The University of Auckland,\\
\small
Private Bag 92019, Auckland 1142, New Zealand \\
\small
$^2$ Center for Cosmology and Particle Physics, Department of Physics,
New York University,\\
\small
4 Washington Place, New York, NY, 10003, USA\\
\small
$^*$\texttt{bj.brewer@auckland.ac.nz}
}

\begin{document}
\maketitle

\abstract{We introduce a fully probabilistic method for producing catalogs
from images of crowded stellar fields. The method is demonstrated on simulated
data where the luminosity function of the stars is a broken power-law.}

\section{Introduction}

A fundamental problem in astronomy is the construction of {\it catalogs} from
raw image data. {\bf The catalogs are then used for...}
Standard tools for generating catalogs include \texttt{SExtractor}
\citep{sextractor}, {\bf more refs}.

However, standard methods for constructing catalogs can have difficulty in some
challenging situations. For example, when multiple sources overlap partially
or completely, it can be difficult to determine how many sources are present,
and how much flux belongs to each source. In principle, uncertainty should be
taken into account. Instead of simply {\it estimating} the position and flux of
each object in the image, we should fully describe the fact that sometimes we
are not certain of the position and flux of each source.

Essentially, the creation of a catalog is an attempt to answer the question
``given the image we have obtained, what objects are present in the field and
what are their properties?''. This motivates a probabilistic (Bayesian)
approach to making catalogs. The output of such an approach would not be a
single answer to this question (i.e. a single catalog), but rather a
probability distribution over the space of possible catalogs.
Constructing such a probability distribution is
challenging for a number of reasons. Firstly, the number $N$ of objects in the
image (and that should be in the catalog) is itself unknown. Secondly, if $N$ is large, then the parameter space
of positions and properties (flux, size, etc) of the objects is also large.
This can cause Markov Chain Monte Carlo (MCMC) algorithms difficulties -- they
may take a long time to converge to the target posterior distribution over
catalogs.

Bayesian object detection has been implemented by \citet{2011MNRAS.415.3462F}
under the assumption of a known number of objects $N$.
This assumption is required for the
{\tt MultiNest} sampler \citep{multinest} to be applicable.
Using the results from the known $N$
run, it is possible (under certain circumstances) to reconstruct what the
results would have been if a variable-$N$ simulation had been used. However,
this will not work well in situations where there is significant confusion
(i.e. two or more sources overlap). What is really required is a variable
dimension model, where $N$ is an unknown quantity to be inferred from the data.
The computational implementation of these models will require tools such as
reversible jump Markov Chain Monte Carlo \citep{rjmcmc}.

\section{Bayesian Inference}
To quantitatively model uncertainties, Bayesian Inference is the appropriate
framework \citep{cox, jaynes, caticha}. Suppose there exist unknown parameters
(denoted
collectively by $\theta$) and we expect to obtain some data $x$. Our prior
state of knowledge about the parameters is modelled by a prior
probability distribution:
\begin{equation}
p(\theta)
\end{equation}
We also model how the parameters give rise to the data, via a generative model.
This is also known as a {\it sampling distribution}.
\begin{equation}
p(x|\theta)
\end{equation}
Despite the singular, the sampling distribution is actually a family of
probability distributions over the data space, one for each possible value
of $\theta$. Note that
the sampling distribution is also an assertion of a prior state of knowledge:
It models prior information about the fact that the data $x$ is connected to
the parameters $\theta$ in some way. Without this prior knowledge, learning is
impossible: there has to be some relationship between the parameters and the
data, otherwise new data tells you about nothing but itself.

When specific data $x^*$ has been obtained, our state of knowledge about $\theta$
gets updated from the prior distribution to the posterior distribution,
via Bayes' rule:
\begin{eqnarray}
p(\theta|x=x^*) &\propto& p(\theta)p(x|\theta)|_{x=x^*} \\
&=& p(\theta)\mathcal{L}(\theta; x)
\end{eqnarray}
The term $p(\theta)p(x|\theta)|_{x=x^*} = \mathcal{L}(\theta; x)$
is the {\it likelihood function}, which is the 
probability of obtaining the actual data set $x^*$ as a function of the
parameters\footnote{In the case that the sampling distribution is a probability
density, the likelihood is the probability density evaluated at the observed
data. This usually causes no problems, although one should be aware of the
Borel-Kolmogorov paradox \citep{jaynes}.}.
As suggested by the above notation, the likelihood function is obtained from the
sampling distribution with the actual data substituted in.

\section{The Specific Model for Stellar Fields}

\subsection{The Hypothesis Space and the Prior}
Our model makes the following assumptions. There are an unknown number of stars
$N$ in the field. Each star has an unknown
position $(x,y)$ in the plane of the sky, and an unknown flux $f$. We also
describe distribution of fluxes (commonly known as the {\it luminosity
function}) of the stars by parameters $\beta$. In summary, the unknown
parameters are:
\begin{eqnarray}
\theta = \left\{N, \beta, \left\{x_i, y_i\right\}_{i=1}^N, 
\left\{f_i\right\}_{i=1}^N\right\}
\end{eqnarray}
The prior probability distribution for the unknown parameters can be factorized
using the product rule. With a variety of independence assumptions, the prior
can be written as:
\begin{eqnarray}
p(\theta) = p(N)p(\beta)\prod_{i=1}^N p(x_i, y_i)
p(f_i | \beta) 
\end{eqnarray}
Here, we have assumed that the luminosity function does not depend on position.
We have also assumed independence of $N$ and $\beta$, which means that
learning the value of one would not tell us anything about the values of the
others. Finally, the fluxes of the stars come independently from
a common distribution. If we knew the luminosity
function of the stars, then the location and flux of a particular star would
not tell us anything about the location and flux of another star. Really, this
is just a way of implementing exchangeability of the stars.

For simplicity, we assume a uniform prior probability distribution for the
position of each star. This creates a strong preference for catalogs where
the stars are uniformly distributed across the image. Thus, this model is appropriate for small patches of sky where the density of stars is approximately
uniform. In other scenarios, such as images of stellar clusters, it is possible
to parameterise the spatial distribution of the stars in a similar way to how
we have parameterised the luminosity function.


\subsection{The Sampling Distribution}
The sampling distribution is a probabilistic model for the process that
generates the data; it describes the probability distribution we would use
to predict the data if we happened to know the true catalog. In our case,
the data will be an $n \times n$ array of pixel intensities:
\begin{eqnarray}
\{Y_{ij}\}
\end{eqnarray}
The center of each pixel is:
\begin{eqnarray}
\{x_{ij}, y_{ij}\}.
\end{eqnarray}
Thus, we need
a prescription for simulating an image $\{Y_{ij}\}$ from a catalog
$\theta = \left\{N, \beta, \left\{x_i, y_i\right\}_{i=1}^N, 
\left\{f_i\right\}_{i=1}^N\right\}$.
If we knew the true catalog, we could compute the
``mock image'' we would expect to see
in the absence of noise. This mock image (at infinite resolution) is given by:
\begin{eqnarray}
\mathcal{M}(x, y) &=& \sum_{i=1}^N \mathcal{P}(x - x_i, y - y_i)
\end{eqnarray}
where $\mathcal{P}$ is the pixel-convolved point spread function (PSF).
Throughout this paper we
assume the pixel-convolved PSF is a known weighted mixture of two concentric
circular
Gaussians with widths $s_1$ and $s_2$:
\begin{eqnarray}
\mathcal{P}(x, y) = \frac{w}{2\pi s_1^2}\exp
\left[
-\frac{1}{2s_1^2}\left(x^2 + y^2\right)
\right]
+ \frac{1-w}{2\pi s_2^2}\exp
\left[-\frac{1}{2s_2^2}\left(x^2 + y^2\right)
\right].
\end{eqnarray}
The finite-resolution observed image is assumed to be generated from the mock
image (evaluated at the center of each pixel) plus Gaussian noise:
\begin{eqnarray}
Y_{ij} &=& \mathcal{M}(x_{ij}, y_{ij}) + \epsilon_{ij}
\end{eqnarray}
where the errors $\{\epsilon_{ij}\}$ are independent and normally distributed:
\begin{eqnarray}
\epsilon_{ij} \sim \mathcal{N}(0, \sigma^2).
\end{eqnarray}
Throughout this paper, we will assume that the noise variance $\sigma^2$ is
known. This is not usually true in practice (although it is often assumed).
However it is straightforward to extend the model so that $\sigma$ is an
unknown parameter.

\subsection{The Prior Distribution}
The prior distribution for the number of stars $N$ is uniform between 0 and
some maximum number $N_{\rm max}$. The extent of the image is assumed to be
from $x=-1$ to $x=1$ and from $y=-1$ to $y=1$ in arbitrary units, and the
positions of the stars are assigned independent uniform priors:
\begin{eqnarray}
x_i &\sim& \textnormal{Uniform}(-1.1, 1.1) \\
y_i &\sim& \textnormal{Uniform}(-1.1, 1.1) \\
\end{eqnarray}
The model for the luminosity function is a broken power-law with four
parameters:
\begin{eqnarray}
\beta = \{h_1, h_2, \alpha_1, \alpha_2\}.
\end{eqnarray}

\section{MCMC Implementation}
The sampling was implemented using the Diffusive Nested Sampling \citep{dnest}
method (hereafter DNS). DNS is based on the Metropolis-Hastings algorithm and
is very generally applicable. The main difference between DNS and the standard
Metropolis-Hastings algorithm is that the target distribution is modified.
Rather than simply exploring the posterior distribution over catalog space,
DNS constructs an alternative target distribution which is a mixture of the
prior distribution with more constrained versions of the prior distribution.
The modified target distribution assists 

Nested Sampling is good because it is invariant under monotonic transformations
of the likelihood function. i.e. the exploration only depends on things like
whether one point is better than another, and not on how much better it is.

\section{A More Realistic Model}
\citep{2008ApJ...682..874K}



\section{Three Scenarios: Sparse, Cluster, and Crowded}
In this paper we discuss three test cases, named \sparse, \cluster
and \crowded. For the generation of the simulated
images, we used the following assumptions:

\begin{table}
\begin{center}
\begin{tabular}{ccccc}
Test Case & Parameter & Value \\
\hline
\sparse \\
\cluster \\
\crowded
\end{tabular}
\end{center}
\end{table}

\begin{figure}
\hspace{-1cm}
\includegraphics{Figures/test_cases.eps}
\end{figure}

\subsection{Sparse}
Sparse has a giant first-order phase transition. {\bf Nested Sampling is
mandatory!} Show the plot of it

\begin{figure}
\begin{center}
\includegraphics[scale=0.7]{Figures/likelihood.eps}
\end{center}
\end{figure}

\subsection{Cluster}
Was generated from non-uniform spatial distribution (obviously) but modelled
with a uniform one.

\subsection{Crowded}
Crowded is slow if the data have high S/N. Something like $e^5000$ compressions
are needed. Can do NS and not worry about equilibration if you are willing to
make a guess about the logXs of the levels.

\subsection{Toy Model}
For the first implementation I wrote at NYU, I used the following model and
priors:
\begin{itemize}
\item $N \sim \textnormal{Uniform}(0, 1, 2, ..., 1000)$ \\
\item The parameters of the spatial
distribution: $\alpha = \left\{x_c, y_c, \sigma\right\}$ \\
\item The spatial distribution: $p(x_i, y_i | \alpha)\sim\mathcal{N}\left(
\texttt{mean}=(x_c, y_c), \texttt{covariance}=
\left(\begin{array}{cc}\sigma^2 & 0 \\ 0 & \sigma^2\end{array}\right)\right)$ \\
\item Prior on $\alpha$: Uniform within image for $(x_c, y_c)$, logUniform for
$\sigma$ with max value $\sim$ image size.
\item The luminosity function: $p(f_i | \beta) \sim
\textnormal{Exponential}\left(\texttt{mean}=\beta\right)$ \\
\item Prior on $\beta$: logUniform with generous bounds.
\end{itemize}





\section{Acknowledgements}
BJB would like to thank his teaching mentor Dr Wayne Stewart.
Marshall, Lang, ...


\appendix

\section{Broken Power-Law Distribution}
The broken power-law distribution is defined by:
\begin{eqnarray}
p(x) &\propto&
\left\{
\begin{array}{lcr}
0, & & x < x_1 \\
x^{-\alpha_1 - 1} & & x_1 \leq x \leq x_2 \\
x^{-\alpha_2 - 1} & & x > x_2
\end{array}
\right.
.
\end{eqnarray}
The free parameters of the broken power-law are:
\begin{eqnarray}
\beta = \{x_1, x_2, \alpha_1, \alpha_2\}
\end{eqnarray}

\begin{figure}
\begin{center}
\includegraphics[scale=0.7]{Figures/broken.eps}
\end{center}
\end{figure}

With normalising terms included, the proportionality becomes an equality:
\begin{eqnarray}
p(x) &=&
\left\{
\begin{array}{lcr}
0, & & x < x_1 \\
Z_1^{-1}x^{-\alpha_1 - 1}, & & x_1 \leq x \leq x_2 \\
Z_2^{-1}x^{-\alpha_2 - 1}, & & x > x_2
\end{array}
\right.
.
\end{eqnarray}

Two conditions will be used to determine the normalisers $Z_1$ and $Z_2$.
Firstly, the PDF should be continuous at $x=x_2$:
\begin{eqnarray}
Z_1^{-1}x_2^{-\alpha_1 - 1} &=& Z_2^{-1}x_2^{-\alpha_2 - 1}\\
\implies
Z_2 &=& Z_1x_2^{\alpha_1-\alpha_2}
\end{eqnarray}

The second condition is the total probability must be 1:

\begin{eqnarray}
\int_{x_1}^{x_2} Z_1^{-1} x^{-\alpha_1 - 1} \, dx
+
\int_{x_2}^\infty Z_2^{-1} x^{-\alpha_2 - 1} \, dx
&=& 1 \\
Z_1^{-1}\alpha_1^{-1}\left[x_1^{-\alpha_1} - x_2^{-\alpha_1}\right]
+
Z_2^{-1}\alpha_2^{-1}x_2^{-\alpha_2}
&=& 1 \\
Z_1^{-1}\alpha_1^{-1}\left[x_1^{-\alpha_1} - x_2^{-\alpha_1}\right]
+
Z_1^{-1}x_2^{\alpha_2-\alpha_1}\alpha_2^{-1}x_2^{-\alpha_2}
&=& 1
\end{eqnarray}

\begin{eqnarray}
\implies
Z_1 &=& \alpha_1^{-1}\left[x_1^{-\alpha_1} - x_2^{-\alpha_1}\right]
+
x_2^{-\alpha_1}\alpha_2^{-1}
\end{eqnarray}

The cumulative distribution (CDF) is:
\begin{eqnarray}
P(X \leq x) = F(x) &=& 
\left\{
\begin{array}{lcr}
0, & & x < x_1 \\
(Z_1\alpha_1)^{-1}\left(x_1^{-\alpha_1} - x^{-\alpha_1}\right), & & x_1 \leq x \leq x_2 \\
1 - (Z_2\alpha_2)^{-1}x^{-\alpha_2}, & & x > x_2 
\end{array}
\right.
\end{eqnarray}
and the inverse CDF is:
\begin{eqnarray}
F^{-1}(u) &=& 
\left\{
\begin{array}{lcr}
\left[x_1^{-\alpha_1} - uZ_1\alpha_1\right]^{-1/\alpha_1}, & & 0 < u < 1 - (Z_2\alpha_2)^{-1}x_2^{-\alpha_2}\\
\left[Z_2\alpha_2(1-u)\right]^{-1/\alpha_2},& & 1 - (Z_2\alpha_2)^{-1}x_2^{-\alpha_2}< u < 1
\end{array}
\right.
\end{eqnarray}




\section{Pixel Convolved PSF}
Consider a ``true image'' $f_0(x, y)$ with infinite resolution. We now model how
this image gives rise to the observed data. First, it is convolved with a
PSF $g(\delta_x, \delta_y)$ (assumed normalized to 1) to give the blurred image:
\begin{eqnarray}
f_{\rm{blurred}}(x, y) &=& \iint f_0(x-\delta_x, y-\delta_y)
g(\delta_x, \delta_y) \, d\delta_x \, d\delta_y \label{blur}
\end{eqnarray}
Then, we observe the blurred image with a certain pixellation. Consider a pixel.
The true flux $F$ within the pixel is:
\begin{eqnarray}
F &=& \iint_{\rm pixel} f_{\rm blurred}(x, y) \,dx'\,dy'\\
&=& \iint f_{\rm blurred}(x, y)\mathbbm{1}\left[(x,y) \in \textnormal{pixel}
\right] \,dx\,dy \label{pixel}
\end{eqnarray}
Now consider $F$ as a function of the central location $(x_c, y_c)$ of the
pixel.




\subsubsection{Proposal Distributions}

\begin{table}
\begin{center}
\begin{tabular}{c|c|c}
Parameter & Proposal & Notes\\
\hline
$N$ & $N \to N + \delta_N$ & Generate $\delta_N$ new stars from prior given
$(\alpha, \beta)$\\
$N$ & $N \to N - \delta_N$ & Remove $\delta_N$ stars at random\\
$\alpha$ & $\alpha \to \alpha + \delta_\alpha$ & Move stars along with $\alpha$
\\
$\alpha$ & $\alpha \to \alpha + \delta_\alpha$ & Stars fixed, use logHastings
factor \\
$\beta$ & $\beta \to \beta + \delta_\beta$ & Move stars' fluxes along with
$\beta$\\
$\beta$ & $\beta \to \beta + \delta_\beta$ & Stars' fluxes fixed, use
logHastings factor \\
$(x,y)$ & $(x,y) \to (x,y)+(\delta_x, \delta_y)$ & Can move $>1$ star.
Include prior via logH \\
$f$ & $f \to f + \delta_f$ & Can move $>1$ stars' fluxes. Include prior
via logH
\end{tabular}
\end{center}
\caption{All $\delta$ parameters are drawn from multi-scale distibutions such
that the largest steps are of order the prior width, and the smallest steps
are of order $10^{-6}$ times the prior width.\label{proposals}}
\end{table}

See Table~\ref{proposals} for a list of current proposal distributions.




%\subsection{Future Possibilities}
%{\bf Hogg}: {\it I think this problem is important enough and general enough
%that we should spend some time working on some ideas there.  If we can sample
%over image explanations, our powers in astronomy will be awesome
%beyond our wildest imaginings.}

%\subsection{Genetic Moves}
%Here I will sketch some ideas...




\begin{thebibliography}{99}
\bibitem[Bertin and 
Arnouts(1996)]{sextractor} Bertin, E., Arnouts, S.\ 1996.\ SExtractor: Software for source extraction.\ Astronomy and Astrophysics Supplement Series 117, 393-404.

\bibitem[\protect\citeauthoryear{Brewer, P{\'a}rtay, 
\& Cs{\'a}nyi}{2011}]{dnest} Brewer B.~J., P{\'a}rtay L.~B.,
Cs{\'a}nyi G., 2011, Statistics and Computing, 21, 4, 649-656. arXiv:0912.2380

\bibitem[Caticha(2009)]{caticha} Caticha, A.\ 2009.\ 
Quantifying Rational Belief.\ American Institute of Physics Conference 
Series 1193, 60-68. 

\bibitem[Cox(1946)]{cox} Cox, R.~T., 1946, Probability, Frequency, and Reasonable Expectation. 1946. American Journal of Physics 14 14, 1-13.

\bibitem[Feroz et al.(2011)]{2011MNRAS.415.3462F} Feroz, F., Balan, S.~T., 
Hobson, M.~P.\ 2011.\ Detecting extrasolar planets from stellar radial 
velocities using Bayesian evidence.\ Monthly Notices of the Royal 
Astronomical Society 415, 3462-3472. 

\bibitem[\protect\citeauthoryear{Feroz, Hobson, 
\& Bridges}{2009}]{multinest} Feroz F., Hobson M.~P., Bridges M., 2009, MNRAS, 398, 1601 

\bibitem[\protect\citeauthoryear{Green}{1995}]{rjmcmc}
Green, P.~J., 1995, Reversible Jump Markov Chain Monte Carlo Computation and Bayesian Model Determination, Biometrika 82 (4): 711â€“732.

\bibitem[Jaynes(2003)]{jaynes} Jaynes, E.~T., 2003, Probability Theory: The
Logic of Science, ISBN 0521592712, Cambridge University Press, June 2003.

\bibitem[Kelly et al.(2008)]{2008ApJ...682..874K} Kelly, B.~C., Fan, X., 
\& Vestergaard, M.\ 2008, ApJ, 682, 874 

\bibitem[Mackay(2003)]{mackay} Mackay, D.~J.~C., 2003, Information Theory,
Inference and Learning Algorithms, Cambridge University Press, UK.

\bibitem[\protect\citeauthoryear{Skilling}{1998}]{massinf} 
Skilling J., 1998, Massive Inference and Maximum Entropy, in Maximum Entropy 
and Bayesian Methods, Kluwer Academic Publishers, Dordrecht/Boston/London p.14
\end{thebibliography}

\end{document}

